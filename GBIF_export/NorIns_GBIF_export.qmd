---
title: "GBIF data export from the Norwegian insect monitoring program (NorIns)"
author: "Jens Åström"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format: gfm
---



```{r}
suppressPackageStartupMessages({
require(tidyverse)
require(tidyjson)
require(xml2)
})
```

```{r setup}
#| include: false
#This is optional
#I choose the 'styler' package for tidying the code to preserve indentations
#I set the cutoff for code tidying to 60, but this doesn't currently work with styler.
#Set tidy = True to get the knitr default
#I want all figures as png and pdf in high quality in a subfolder called figure 

knitr::opts_chunk$set(echo = TRUE, 
                      tidy = "styler",
                      dev = c("png", "pdf"),
                      dpi = 600,
                      fig.path = "figure/",
                      tidy.opts = list(width.cutoff = 60)
                      )

options(xtable.comment = F, 
        xtable.include.rownames = F, 
        nina.logo.y.pos = 0.15)
#palette(ninaPalette())
```


## Introduction

This script downloads observation records from the Norwegian insect monitoring program (NorIns), and unpacks the data for further use. The data is stored within GBIF, and the first section shows how to download the data from there. The rest of the script deals with rearranging the data from GBIFs eventCore standard, where several hierarchical levels is compressed into a single event table. Portions of the script can be used as an example for projects with other data formatting preferences. The data is freely available, given that you site the source appropriately. See "Citation and meta-data" below.

## Data structure

The data collection of the program has a hierarchical nature, and the data from each level is originally stored in separate tables in a database (normalized data). At the topmost level we have 4) `year_locality`, sampling seasons (april-october) of localities, where we store data that is common for the whole locality-season, such as habitat quality, locality name etc. At level 3) `locality_sampling`, we have individual sampling periods of typically 2 weeks within each locality-season. Each season can have up to 14 such sampling periods, and this data includes sampling duration and associated weather. At level 2) `sampling_trap`, we have data from a single trap within a sampling period, such as biomass and ethanol concentration. Some localities have multiple traps, so there can be several rows of trap data `sampling_trap` for a single `locality_sampling`. Lastly, at level 1 `identifications`, we have insect identifications of each trap sample. Most samples only have 1 identification "event", although multiple identification methods can be employed for some samples.

The hierarchical structure is handled within GBIF through the Darwin Event Core-standard. Here, all observation data of the species are located in an "occurrence"-table. This stores the species names and amounts that has been observed. This table is linked to an "event"-table containing all the information about the observations, such as when and where it was made. This single event-table has to handle all the hierarchical levels mentioned above, and thus has to combine data from many original tables. The hierarchical nature of the data is maintained through "parent-event"-keys, which relate lower levels to their respective higher "parent" levels. By this design of GBIF, you can have an arbitrary number of parent events (hierarchical levels) within the same table, instead of spreading the data across several tables (normalizing the data). This data structure is therefore very flexible and compact. The downside is that you need to keep track of the eventID's and the parentEventID's , and unpack the data to recreate the original data structure.

This prosess is shown here.


## Finding the raw data and how to cite it

The dataset can be found at http://gbif.org by searching for "National insect monitoring in Norway". This will take you to the webpage of the dataset: https://www.gbif.org/dataset/19fe96b0-0cf3-4a2e-90a5-7c1c19ac94ee where you can access the meta-data of the dataset. The data is freely available, but please cite the source. The citation is visible at the dataset's webpage at GBIF. The citation can also be fetched programmatically this way.

```{r, eval = T}
dataset_id <- "19fe96b0-0cf3-4a2e-90a5-7c1c19ac94ee" ##From the webpage URL
# Suggested citation: Take the citation as from downloaded from GBIF website, replace "via GBIF.org" by endpoint url. 
tmp <- tempfile()
download.file(paste0("http://api.gbif.org/v1/dataset/",dataset_id,"/document"),tmp) # get medatadata from gbif api
meta <- read_xml(tmp) %>% as_list() # create list from xml schema
gbif_citation <- meta$eml$additionalMetadata$metadata$gbif$citation[[1]] # extract citation
```

The citation to use is then:

* `r paste(gbif_citation)`


## Fetching the raw-data from GBIF

We need an "endpoint_url" to retrieve the actual data set. This is found at the bottom of the webpage for the dataset. This can also be found programmatically as shown here.

```{r} 
#| eval: TRUE
dataset_url <-  paste0("http://api.gbif.org/v1/dataset/",dataset_id,"/endpoint")
dataset <- RJSONIO::fromJSON(dataset_url)
endpoint_url <- dataset[[1]]$url # extracting URL from API call result
```
The endpoint for our dataset looks like this: (Also visible on the webpage for the dataset)

* https://ipt.nina.no/archive.do?r=national_insect_monitoring


With the endpoint, we can download the data in a zipped format.
```{r}
#| eval: false
# Download from dwc-a from IPT  putting into data folder
system("mkdir -p GBIF_data")
download.file(endpoint_url, destfile = "GBIF_data/gbif_download.zip", mode="wb")

```
We unzip it in the same folder.
```{r}
#| eval: false
unzip("GBIF_data/gbif_download.zip",
      exdir = "GBIF_data")
```


### Read in the raw GBIF files

The data is a simple tab delimited text file. 

```{r}
#| eval: true
event_raw <- read_delim("GBIF_data/event.txt", 
                       delim = "\t",
                       locale = locale(encoding = "UTF-8"),
                       progress = FALSE,
                       show_col_types = FALSE)
```

The occurrence data follows the same procedure.
```{r}
#| eval: true
occurrence_raw <- read_delim("GBIF_data/occurrence.txt", 
                       delim = "\t",
                       locale = locale(encoding = "UTF-8"),
                       progress = FALSE,
                       show_col_types = FALSE
                       )

```


## Unpacking the data from the GBIF raw format

As mentioned above, the GBIF eventCore data format is compact for efficient storage, but isn't the most practical to use for analysis and exploration. The next step is therefore to split out the separate levels from the combined event table into separate tables. Optionally, these can later be joined to create single (but very wide) data table. I will use the original table names for each hierarchical level.

We start by splitting up the 4 different levels.

```{r}
identifications_raw <- event_raw %>% 
  filter(samplingProtocol == "Level_1")

sampling_trap_raw <- event_raw %>% 
  filter(samplingProtocol == "Level_2")

locality_sampling_raw <- event_raw %>% 
  filter(samplingProtocol == "Level_3")

year_locality_raw <- event_raw %>% 
  filter(samplingProtocol == "Level_4")

```


### Extract the data that didn't fit into the GBIF standard

In all levels, we have information that don't fit into the GBIF data format/ontology. These are bundled together in to a JSON-string in a darwinCore column named "dynamicProperties". We need to spread this content into separate columns.

```{r}
identifications_raw %>% 
  select(dynamicProperties) %>% 
  head(1)
```

Here, we split out the json string (dynamicProperties) into a separate table, and join it back to the original data afterwards. I tried various json-packages earlier with limited success, but this works and should be stable.

#### Identifications
To reiterate, this is the species identifications (metabarcoding results) from each trap catch. As of now (2023), there is only 1 species identification process per sample, but in principle there could be multiple identifications for some samples, for example manual identification in addition to metabarcoding.

```{r}

identifications <- identifications_raw %>% 
  #collect() %>% 
  select(-dynamicProperties)

tempDynamic <- identifications_raw %>% 
  #head() %>% 
  select(dyn = dynamicProperties) %>% 
  #collect() %>% 
  unlist() %>% 
  spread_all()  %>% 
  as_tibble() %>% 
  select(-document.id)

identifications <- identifications %>% 
  left_join(tempDynamic,
            by = c("id" = "id"))

if(!nrow(identifications) == nrow(identifications_raw) & nrow(identifications) == nrow(tempDynamic)) stop("Identification rows don't match!")

```

The new columns contain an identification method name, optional identification comment, read_abundance (number of DNA-copies), extraction date, extraction-kit, and optional extraction comment. Here is a sample of the extracted columns, added to the rest of the data:
```{r}
identifications %>% 
  select(identification_name,
         identification_comment,
         read_abundance,
         ekstraksjonsdato,
         ekstraksjonskit,
         ekstraksjonskommentar) %>% 
  head()
```

We here also remove some mandatory darwinCore-columns that are not particularly relevant for this level. We also rename the parentEventID column to make further joins more intuitive, and adopt a snake case naming standard, a good practice when working with databases (esp. PostgreSQL). 

```{r}
identifications <- identifications %>% 
  select(identification_id = eventID,
         sampling_trap_id = parentEventID,
         identification_name,
         identification_comment,
         read_abundance,
         ekstraksjonsdato,
         ekstraksjonskit,
         ekstraksjonskommentar)
```


#### Sampling trap
These are the data attributed to a single trap catch. The locationIDs (UUIDs) are not very human-readable, but we can extract the original trap name from the "locationRemarks" column.

```{r}
sampling_trap <- sampling_trap_raw %>% 
  mutate(trap_name = stringr::str_split_i(locationRemarks, ": ", 2)) %>% 
  select(-dynamicProperties) 

tempDynamic <- sampling_trap_raw %>% 
  select(dyn = dynamicProperties) %>% 
  unlist() %>% 
  spread_all()  %>% 
  as_tibble() %>% 
  select(-document.id)

sampling_trap <- sampling_trap %>% 
  left_join(tempDynamic,
            by = c("id" = "id"))

if(!nrow(sampling_trap) == nrow(sampling_trap_raw) & nrow(sampling_trap) == nrow(tempDynamic)) stop("Sampling trap rows don't match!")

```

The new columns contain the trap type, trap model, preservation liquid information, measured ethanol concentration at lab (after topping-up in the field), and the biomass of the sample (as wet weight in grams). The samples are weighed in the bottles (gross weight), after a standardized draining of preservation liquid, and the bottle weights (calculated from a sample of 10 empty similar bottles) are subtracted, to get the net wet weight. Note that for the window traps, the collected biomass is too small to measure precisely, and they should be interpreted with caution. For example, the variation in (empty) bottle weights can be larger than the catches, resulting in negative values for net_wet_weight. From 2021 and onwards, the 4 window traps are combined before further prossessing, so the (total weight) and catches are only reported for one of the traps. Trap catches should therefore be summed for the window traps, within locality_samplings. Also note that the ethanol measurement and/or sample weights are missing for a few samples, by lab routine mistakes, particularly for the starting year.

Here is a sample of the extracted columns, added to the rest of the data:

```{r}
sampling_trap %>% 
  select(trap_type,
         trap_model,
         liquid_name,
         ethanol_prc_at_lab,
         net_wet_weight) %>% 
  head()
```

Lastly, we strip away some (for most people) superfluous columns.

```{r}
sampling_trap <- sampling_trap %>% 
  select(sampling_trap_id = eventID,
         locality_sampling_id = parentEventID,
         trap_name,
         sample_size_value = sampleSizeValue,
         sample_size_unit = sampleSizeUnit,
         trap_type,
         trap_model,
         liquid_name,
         ethanol_prc_at_lab,
         net_wet_weight,
         point_latitude = decimalLatitude,
         point_longitude = decimalLongitude,
         point_wkt = footprintWKT,
         coord_unc_in_meters = coordinateUncertaintyInMeters,
         coordinate_system = geodeticDatum)
```


### Locality samplings
These are the data attributed to each locality sampling, meaning a single sampling period in a single location, typically around 2 weeks. The semi-natural sites typically have only 1 sampling trap per locality sampling, but forest habitats have an additional 4 window traps within each locality sampling, thus sharing the same locality sampling information. This level contains information of sampling period and (an aggregate of the) local weather data for the sampling period, collected from loggers on the malaisetrap.

You may note that some locality samplings are missing starting and end times. The bulk of these are planned activity for 2023, which hadn't been performed yet at the time of export to GBIF. In addition, some locality sampling events of past years have been skipped or missed, and are not connected to any observational data. One reason for skipping sampling numbers is that some localities were started early, and this was later corrected so that the dates and sample numbers would line up across localities. Including these "ghost" sampling events is a mistake in the export, and they will be eliminated in later dataset versions.

Lastly, there are 15 "true" missing values for start and end dates. This pertains to the region "Østlandet/Ost" for the sampling numbers 3 and 4 in the localites Semi-nat_17, Semi-nat_18, Semi-nat_19, Semi-nat_20, Skog_12, Skog_15, Skog_16, and Skog_20. These will be found and amended, after control of the stored physical samples.


```{r}
locality_sampling <- locality_sampling_raw %>% 
  mutate(sampling_event = stringr::str_split_i(locationRemarks, ": ", 2)) %>% 
  select(-dynamicProperties) 

tempDynamic <- locality_sampling_raw %>% 
  select(dyn = dynamicProperties) %>% 
  unlist() %>% 
  spread_all()  %>% 
  as_tibble() %>% 
  select(-document.id)

locality_sampling <- locality_sampling %>% 
  left_join(tempDynamic,
            by = c("id" = "id"))

if(!nrow(locality_sampling) == nrow(locality_sampling_raw) & nrow(locality_sampling) == nrow(tempDynamic)) stop("Locality sampling rows don't match!")

```

The GBIF format has one single column for event time, where we have stored both the start and the end times of the samplings. These need to be split into separate columns. Note that the sampling times are recorded as whole days, and the hours are implicitly set to midnight (in UTC + 2hrs).

```{r}
locality_sampling <- locality_sampling %>% 
    separate(eventTime, 
           c("start_time", "end_time"), 
           sep = "/") %>% 
  mutate(start_time = as.POSIXct(start_time),
         end_time = as.POSIXct(end_time))

locality_sampling %>% 
  select(start_time,
         end_time)

```

```{r}
locality_sampling %>% 
  select(start_time,
         end_time,
         sampling_min_temp,
         sampling_max_temp,
         sampling_avg_temp) %>% 
  head()
```

Finally, trimming away some columns.

```{r}
locality_sampling <- locality_sampling %>% 
  select(locality_sampling_id = eventID,
         year_locality_id = parentEventID,
         sampling_event,
         sample_size_value = sampleSizeValue,
         sample_size_unit = sampleSizeUnit,
         start_time,
         end_time,
         sampling_min_temp,
         sampling_max_temp,
         sampling_avg_temp)
```


#### Year localities
Lastly, the year-locality level hold the information that is associated with a whole sampling season for a locality. This includes information on the locality such as name, habitat type, and geographical region. In addition, we include ANO-id (areal representative nature survey) and a selection of the associated flora characteristics, as well as selection of the measured forest characteristics in forest habitat.

This level also contains the mandatory darwinCore "locality" column, which is the nearest population center. We here rename this column to locality_vernacular_name to is won't clash with our own locality codes, stored in the dynamicProperties. We also reformat the SSB-ids (the 500x500 square IDs from the Central statistical bureau) to character format. These codes can be used to merged the data with various national statistics (not shown here).


```{r}
year_locality <- year_locality_raw %>% 
   select(locality_vernacular_name = locality,
         everything()) %>% 
  select(-dynamicProperties)

tempDynamic <- year_locality_raw %>% 
  select(dyn = dynamicProperties) %>% 
  unlist() %>% 
  spread_all()  %>% 
  as_tibble() %>% 
  select(-document.id)


year_locality <- year_locality %>% 
  left_join(tempDynamic,
            by = c("id" = "id"))

if(!nrow(year_locality) == nrow(year_locality_raw) & nrow(year_locality) == nrow(tempDynamic)) stop("Year locality rows don't match!")

```

This level also contains sample time, here meaning the time span of the whole sampling season. This can be split into start and end-times as with the locality samplings.

```{r}
year_locality <- year_locality %>% 
    separate(eventTime, 
           c("start_time", "end_time"), 
           sep = "/") %>% 
  mutate(season_start_time = as.POSIXct(start_time),
         season_end_time = as.POSIXct(end_time)) %>%  
  mutate(ssbid = as.character(ssbid)) 

year_locality %>% 
  select(start_time,
         end_time)
```

Here is a sample of the data at the year locality level. If some starting and end times are missing, these can be inferred (will be fixed in the next data version) from the start and end of the first and last locality samplings, respectively.


```{r}
year_locality %>% 
  select(season_start_time,
         season_end_time,
         locality,
         year,
         habitat_type,
         region_name,
         ssbid,
         ano_flate_id,
         no_herb_spec,
         avg_prc_cov_herb_species,
         no_tree_spec,
         dom_tree_spec,
         avg_dom_tree_age) %>% 
  head()
```

Triming away some columns.

```{r}
year_locality <- year_locality %>% 
  select(year_locality_id = eventID,
         season_sample_size_value = sampleSizeValue,
         season_sample_size_unit = sampleSizeUnit,
         season_start_time,
         season_end_time,
         locality,
         year,
         habitat_type,
         region_name,
         ssbid,
         ano_flate_id,
         no_herb_spec,
         avg_prc_cov_herb_species,
         no_tree_spec,
         dom_tree_spec,
         avg_dom_tree_age,
         centroid_latitude = decimalLatitude,
         centroid_longitude = decimalLongitude,
         polygon_wkt = footprintWKT,
         coordinate_system = geodeticDatum
         )
```

## The occurrence table

We have now come to the occurrence table, which holds the data on the actual species observations. This table can be linked to the accompaning data through the identification table. First, we rename some columns from the GBIF standard. Note that I've been informed that these taxonID-data refer to "scientific name ids" (used by the norwegian species information center), and not proper "taxon-ids" used by GBIF. I therefore correct the naming here. These data might change in the future, adopting the gbif taxonIDs. 

```{r}
occurrence <- occurrence_raw %>% 
  select(occurrence_id = occurrenceID,
         identification_id = eventID,
         quantity = organismQuantity,
         quantity_type = organismQuantityType,
         scientific_name = scientificName,
         vernacular_name = vernacularName,
         scientific_name_id = taxonID,
         class,
         order,
         family,
         genus,
         specific_epithet = specificEpithet)
     
```


## Merging and using

At this point, the data is (mostly) normalized, and ready to be used or reformatted in whatever form you choose. For example, the separate tables could be put into a database, and later joined together by their respective ids, potentially in permanent database views. The ids and qualitative values could optionally be constrained through foreign keys, but this should not be needed. 

Alternatively, we could continue in for example R, and join the tables together manually. By the naming conventions used, this should be self-explanatory but even so, here is a table outlining the tables and the columns that can be used to join them together.

```{r}
key_table <- tibble("Table" = c("occurrence",
                                "identifications",
                                "sampling_trap",
                                "locality_sampling",
                                "year_locality"),
                    "Primary key" = c("occurrence_id",
                                      "identification_id",
                                      "sampling_trap_id",
                                      "locality_sampling_id",
                                      "year_locality_id"),
                    "Foreign key" = c("identification_id",
                                      "sampling_trap_id",
                                      "locality_sampling_id",
                                      "year_locality_id",
                                      ""),
                    "Links to table" =  c("identifications",
                                          "sampling_trap",
                                          "locality_sampling",
                                          "year_locality",
                                          ""),
                    "Links to table column" = c("identification_id",
                                      "sampling_trap_id",
                                      "locality_sampling_id",
                                      "year_locality_id",
                                      ""))

knitr::kable(key_table)
```

## Write the data to disk

This is trivial but shown for completeness. Here we place the tables in separate csv files.

```{r}
system("mkdir -p 'out'")

write_csv(occurrence,
          file = "out/occurrence.csv")
write_csv(identifications,
          file = "out/identifications.csv")
write_csv(sampling_trap,
          file = "out/sampling_trap.csv")
write_csv(locality_sampling,
          file = "out/locality_sampling.csv")
write_csv(year_locality,
          file = "out/year_locality.csv")

```



## Usage examples

To exemplify the usage, we will here join together and perform a few simple summary figures.

### Within season biomass trends
We here join the sampling trap, locality sampling and year_locality together (we don't need the identification and occurrence tables). We will only consider malaisetrap catches to standardize the catch effort between habitat types.

```{r}
biomass_join <- sampling_trap %>% 
  left_join(locality_sampling,
            by = c("locality_sampling_id" = "locality_sampling_id")) %>%
  left_join(year_locality,
            by = c("year_locality_id" = "year_locality_id")) %>% 
  filter(trap_type == "Malaise") %>% 
  mutate(julian_day = lubridate::yday(end_time),
         year = forcats::as_factor(year))
  
```

And a simple phenology plot of the biomass throughout the season, measured at collection time.

```{r}
ggplot(aes(y = net_wet_weight, 
           x = julian_day,
           group = year),
       data = biomass_join) +
  geom_point(aes(color = year)) + 
  geom_smooth(aes(color = year)) + 
  ylab("Wet weight (g.)") +
  xlab("Julian day") +
  scale_color_discrete(name = "Year") +
  facet_wrap(facets = vars(region_name),
             ncol = 1) 
```


### Species richness per locality

We here join also the occurrence table through the identification table, to link observations to localities. Again, we only show malaisetraps to keep the comparison valid between habitat types. 

```{r}
spec_join <- occurrence %>% 
  left_join(identifications,
            by = c("identification_id" = "identification_id")) %>% 
  left_join(sampling_trap,
            by = c("sampling_trap_id" = "sampling_trap_id")) %>% 
  left_join(locality_sampling,
            by = c("locality_sampling_id" = "locality_sampling_id")) %>%
  left_join(year_locality,
            by = c("year_locality_id" = "year_locality_id")) %>% 
  filter(trap_type == "Malaise") %>% 
  mutate(julian_day = lubridate::yday(end_time),
         year = forcats::as_factor(year))
  
```

We can summarize this as the total number of distinct species in a locality, year, and habitat type to get a more manageable table.

```{r}
spec_agg <- spec_join %>% 
  group_by(year,
           region_name,
           locality,
           habitat_type) %>% 
  summarize(no_spec = n_distinct(scientific_name))
```

We then create a basic plot of the total number of species found, divided by habitat type, region, and year. Note that we only export species names with identification confidence = "HIGH" to GBIF. See the reports for documentation of identification classifications.

```{r}
ggplot(aes(y = no_spec,
           x = habitat_type,
           fill = habitat_type),
       data = spec_agg) +
  geom_bar(stat = "sum",
           show_guide = FALSE) +
  scale_fill_discrete(name = "Habitat type") +
  ylab("No. species") +
  xlab("Habitat type") +
  facet_wrap(vars(region_name, year),
             shrink = TRUE,
             drop = TRUE)

```

This concludes the tour of the basic wrangling of this dataset.


